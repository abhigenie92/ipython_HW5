{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision:  Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Science: COMS W 4995 004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: April 6, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: Telling Cats from Dogs using VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This assignment is based on the blog post\n",
    "\"Building powerful image classification models using very little data\"\n",
    "from blog.keras.io. Here you will build a classifier that can distinguish between pictures of dogs and cats. You will use a ConvNet (VGG16) that was pre-trained ImageNet. Your task will be to re-architect the network to solve your problem. To do this you will:\n",
    "0. Make a training dataset, using images from the link below, with 10,000 images of cats and 10,000 images of dogs. Use 1,000 images of each category for your validation set. The data should be orgainized into folders named ./data/train/cats/ + ./data/train/dogs/ + ./data/validation/cats/ + ./data/validation/dogs/. (No need to worry about a test set for this assignment.)  \n",
    "1. take VGG16 network architecture\n",
    "2. load in the pre-trained weights from the link below for all layers except the last layers \n",
    "3. add a fully connected layer followed by a final sigmoid layer to replace the 1000 category softmax layer that was used when the network was trained on ImageNet\n",
    "4. freeze all layers except the last two that you added\n",
    "5. fine-tune the network on your cats vs. dogs image data\n",
    "6. evaluate the accuracy\n",
    "7. unfreeze all layers\n",
    "8. continue fine-tuning the network on your cats vs. dogs image data\n",
    "9. evaluate the accuracy\n",
    "10. comment your code and make sure to include accuracy, a few sample mistakes, and anything else you would like to add\n",
    "\n",
    "Downloads:\n",
    "1. You can get your image data from:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data. \n",
    "2. You can get your VGG16 pre-trained network weights from \n",
    "https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3\n",
    "\n",
    "(Note this assignment deviates from blog.keras.io in that it uses more data AND performs the fine-tuning in two steps: first freezing the lower layers and then un-freezing them for a final run of fine-tuning. The resulting ConvNet gets more than 97% accuracy in telling pictures of cats and dogs apart.)\n",
    "\n",
    "A bunch of code and network definition has been included to to get you started. This is not meant to be a difficult assignment, as you have your final projects to work on!  Good luck and have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we import necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we make the last layer or layers. We flatten the output from the last convolutional layer, and add fully connected layer with 256 hidden units. Finally, we add the output layer which is has a scalar output as we have a binary classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icarus/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load all the packages \n",
    "import os\n",
    "import h5py,pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import time, pickle, pandas\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras import backend\n",
    "from keras import optimizers\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 2 # number of classes\n",
    "class_name = {\n",
    "    0: 'cat',\n",
    "    1: 'dog',\n",
    "}\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = './data/train'\n",
    "validation_data_dir = './data/validation'\n",
    "nb_train_samples = 20000\n",
    "nb_validation_samples = 2000\n",
    "\n",
    "\n",
    "def build_vgg16(framework='tf'):\n",
    "    if framework == 'th':\n",
    "        # build the VGG16 network in Theano weight ordering mode\n",
    "        backend.set_image_dim_ordering('th')\n",
    "    else:\n",
    "        # build the VGG16 network in Tensorflow weight ordering mode\n",
    "        backend.set_image_dim_ordering('tf')\n",
    "\n",
    "    model = Sequential()\n",
    "    if framework == 'th':\n",
    "        model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "    else:\n",
    "        model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    return model\n",
    "\n",
    "weights_path = 'vgg16_weights.h5'\n",
    "th_model = build_vgg16('th')\n",
    "\n",
    "assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n",
    "f = h5py.File(weights_path)\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(th_model.layers):\n",
    "        # we don't look at the last (fully-connected) layers in the savefile\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    th_model.layers[k].set_weights(weights)\n",
    "f.close()\n",
    "print('Model loaded.')\n",
    "\n",
    "tf_model = build_vgg16('tf')\n",
    "\n",
    "# transfer weights from th_model to tf_model\n",
    "for th_layer, tf_layer in zip(th_model.layers, tf_model.layers):\n",
    "    if th_layer.__class__.__name__ == 'Convolution2D':\n",
    "      kernel, bias = th_layer.get_weights()\n",
    "      kernel = np.transpose(kernel, (2, 3, 1, 0))\n",
    "      tf_layer.set_weights([kernel, bias])\n",
    "    else:\n",
    "      tf_layer.set_weights(tf_layer.get_weights())\n",
    "\n",
    "num_layers_before_top=len(tf_model.layers)\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=tf_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "''' We add this model to the top of our VGG16 network, freeze all the weights except the top, and compile.\n",
    "'''\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "tf_model.add(top_model)\n",
    "# freeze all the weights except the top\n",
    "for layer in tf_model.layers[:num_layers_before_top]:\n",
    "    layer.trainable = False\n",
    "tf_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "'''Defining options for data augmentation.'''\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "415s - loss: 0.4650 - acc: 0.7989 - val_loss: 0.3382 - val_acc: 0.8485\n",
      "Epoch 2/5\n",
      "403s - loss: 0.3616 - acc: 0.8428 - val_loss: 0.3205 - val_acc: 0.8630\n",
      "Epoch 3/5\n",
      "403s - loss: 0.3381 - acc: 0.8553 - val_loss: 0.3095 - val_acc: 0.8575\n",
      "Epoch 4/5\n",
      "402s - loss: 0.3289 - acc: 0.8636 - val_loss: 0.3089 - val_acc: 0.8665\n",
      "Epoch 5/5\n",
      "404s - loss: 0.3208 - acc: 0.8673 - val_loss: 0.3115 - val_acc: 0.8665\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Fine-tune the model\n",
    "Now we train for 5 epochs to get the weights for the top close to where we need them. Essentially, we want the network to be doing the right thing before we unnfreeze the lower weights.\n",
    "'''\n",
    "nb_epoch=5 # nuumber of epochs\n",
    "batch_size = 16 # batch-size\n",
    "hist_little_convet = tf_model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nb_train_samples,\n",
    "        nb_epoch = nb_epoch,\n",
    "        validation_data = validation_generator,\n",
    "        nb_val_samples = nb_validation_samples,\n",
    "        verbose=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "472s - loss: 0.2752 - acc: 0.8882 - val_loss: 0.2487 - val_acc: 0.9010\n",
      "Epoch 2/15\n",
      "471s - loss: 0.2298 - acc: 0.9051 - val_loss: 0.2260 - val_acc: 0.9060\n",
      "Epoch 3/15\n",
      "471s - loss: 0.2036 - acc: 0.9178 - val_loss: 0.2216 - val_acc: 0.9060\n",
      "Epoch 4/15\n",
      "471s - loss: 0.1859 - acc: 0.9231 - val_loss: 0.2018 - val_acc: 0.9120\n",
      "Epoch 5/15\n",
      "471s - loss: 0.1722 - acc: 0.9295 - val_loss: 0.2078 - val_acc: 0.9165\n",
      "Epoch 6/15\n",
      "471s - loss: 0.1590 - acc: 0.9373 - val_loss: 0.1690 - val_acc: 0.9270\n",
      "Epoch 7/15\n",
      "471s - loss: 0.1509 - acc: 0.9388 - val_loss: 0.1893 - val_acc: 0.9265\n",
      "Epoch 8/15\n",
      "471s - loss: 0.1409 - acc: 0.9433 - val_loss: 0.1835 - val_acc: 0.9265\n",
      "Epoch 9/15\n",
      "471s - loss: 0.1338 - acc: 0.9483 - val_loss: 0.1990 - val_acc: 0.9225\n",
      "Epoch 10/15\n",
      "471s - loss: 0.1258 - acc: 0.9508 - val_loss: 0.1811 - val_acc: 0.9275\n",
      "Epoch 11/15\n",
      "470s - loss: 0.1171 - acc: 0.9533 - val_loss: 0.1960 - val_acc: 0.9265\n",
      "Epoch 12/15\n",
      "471s - loss: 0.1105 - acc: 0.9569 - val_loss: 0.1824 - val_acc: 0.9300\n",
      "Epoch 13/15\n",
      "471s - loss: 0.1074 - acc: 0.9571 - val_loss: 0.2042 - val_acc: 0.9225\n",
      "Epoch 14/15\n",
      "470s - loss: 0.1012 - acc: 0.9592 - val_loss: 0.2170 - val_acc: 0.9235\n",
      "Epoch 15/15\n",
      "471s - loss: 0.0947 - acc: 0.9638 - val_loss: 0.2348 - val_acc: 0.9135\n"
     ]
    }
   ],
   "source": [
    "''' Set the last conv. block to trainable. That is unfreezing it.'''\n",
    "for layer in tf_model.layers[:num_layers_before_top]:\n",
    "    layer.trainable = False\n",
    "for layer in tf_model.layers[25:num_layers_before_top]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Hey! I just wanted to ask, is there something we can \n",
    "tf_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nb_epoch=15 # training for 15 epochs\n",
    "hist_little_convet = tf_model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nb_train_samples,\n",
    "        nb_epoch = nb_epoch,\n",
    "        validation_data = validation_generator,\n",
    "        nb_val_samples = nb_validation_samples,\n",
    "        verbose=2\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "470s - loss: 0.0567 - acc: 0.9781 - val_loss: 0.2156 - val_acc: 0.9380\n",
      "Epoch 2/10\n",
      "470s - loss: 0.0576 - acc: 0.9783 - val_loss: 0.2121 - val_acc: 0.9380\n",
      "Epoch 3/10\n",
      "470s - loss: 0.0489 - acc: 0.9811 - val_loss: 0.1992 - val_acc: 0.9395\n",
      "Epoch 4/10\n",
      "471s - loss: 0.0500 - acc: 0.9811 - val_loss: 0.1936 - val_acc: 0.9430\n",
      "Epoch 5/10\n",
      "471s - loss: 0.0488 - acc: 0.9808 - val_loss: 0.2130 - val_acc: 0.9370\n",
      "Epoch 6/10\n",
      "471s - loss: 0.0447 - acc: 0.9832 - val_loss: 0.2152 - val_acc: 0.9405\n",
      "Epoch 7/10\n",
      "470s - loss: 0.0443 - acc: 0.9846 - val_loss: 0.2115 - val_acc: 0.9385\n",
      "Epoch 8/10\n",
      "471s - loss: 0.0416 - acc: 0.9834 - val_loss: 0.2040 - val_acc: 0.9410\n",
      "Epoch 9/10\n",
      "471s - loss: 0.0410 - acc: 0.9851 - val_loss: 0.2473 - val_acc: 0.9410\n",
      "Epoch 10/10\n",
      "471s - loss: 0.0397 - acc: 0.9860 - val_loss: 0.2522 - val_acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "'''Unfreezing all layers and training the whole network with backprop.'''\n",
    "for layer in tf_model.layers: # unfreezing all layers\n",
    "    layer.trainable = True\n",
    "nb_epoch=10\n",
    "hist_little_convet = tf_model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nb_train_samples,\n",
    "        nb_epoch = nb_epoch,\n",
    "        validation_data = validation_generator,\n",
    "        nb_val_samples = nb_validation_samples,\n",
    "        verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "471s - loss: 0.0356 - acc: 0.9873 - val_loss: 0.2346 - val_acc: 0.9340\n",
      "Epoch 2/10\n",
      "471s - loss: 0.0377 - acc: 0.9866 - val_loss: 0.2152 - val_acc: 0.9390\n",
      "Epoch 3/10\n",
      "471s - loss: 0.0327 - acc: 0.9886 - val_loss: 0.2168 - val_acc: 0.9415\n",
      "Epoch 4/10\n",
      "471s - loss: 0.0324 - acc: 0.9888 - val_loss: 0.2439 - val_acc: 0.9435\n",
      "Epoch 5/10\n",
      "471s - loss: 0.0291 - acc: 0.9892 - val_loss: 0.2326 - val_acc: 0.9425\n",
      "Epoch 6/10\n",
      "471s - loss: 0.0340 - acc: 0.9880 - val_loss: 0.2337 - val_acc: 0.9400\n",
      "Epoch 7/10\n",
      "471s - loss: 0.0305 - acc: 0.9891 - val_loss: 0.2238 - val_acc: 0.9450\n",
      "Epoch 8/10\n",
      "471s - loss: 0.0283 - acc: 0.9901 - val_loss: 0.2658 - val_acc: 0.9340\n",
      "Epoch 9/10\n",
      "471s - loss: 0.0273 - acc: 0.9905 - val_loss: 0.2182 - val_acc: 0.9470\n",
      "Epoch 10/10\n",
      "471s - loss: 0.0290 - acc: 0.9892 - val_loss: 0.2308 - val_acc: 0.9415\n"
     ]
    }
   ],
   "source": [
    "'''Training the network for 10 more epochs.'''\n",
    "hist_little_convet = tf_model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nb_train_samples,\n",
    "        nb_epoch = nb_epoch,\n",
    "        validation_data = validation_generator,\n",
    "        nb_val_samples = nb_validation_samples,\n",
    "        verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1325s - loss: 0.0215 - acc: 0.9927 - val_loss: 0.2132 - val_acc: 0.9485\n",
      "Epoch 2/20\n",
      "1317s - loss: 0.0203 - acc: 0.9927 - val_loss: 0.1965 - val_acc: 0.9515\n",
      "Epoch 3/20\n",
      "1316s - loss: 0.0175 - acc: 0.9937 - val_loss: 0.2081 - val_acc: 0.9520\n",
      "Epoch 4/20\n",
      "1317s - loss: 0.0161 - acc: 0.9940 - val_loss: 0.2634 - val_acc: 0.9400\n",
      "Epoch 5/20\n",
      "1317s - loss: 0.0156 - acc: 0.9947 - val_loss: 0.2077 - val_acc: 0.9560\n",
      "Epoch 6/20\n",
      "1317s - loss: 0.0171 - acc: 0.9936 - val_loss: 0.2181 - val_acc: 0.9475\n",
      "Epoch 9/20\n",
      "1316s - loss: 0.0163 - acc: 0.9937 - val_loss: 0.2341 - val_acc: 0.9450\n",
      "Epoch 10/20\n",
      "1316s - loss: 0.0142 - acc: 0.9960 - val_loss: 0.2313 - val_acc: 0.9490\n",
      "Epoch 11/20\n",
      "1316s - loss: 0.0147 - acc: 0.9952 - val_loss: 0.1706 - val_acc: 0.9545\n",
      "Epoch 12/20\n",
      "1316s - loss: 0.0157 - acc: 0.9942 - val_loss: 0.2264 - val_acc: 0.9510\n",
      "Epoch 13/20\n",
      "1316s - loss: 0.0132 - acc: 0.9956 - val_loss: 0.2096 - val_acc: 0.9535\n",
      "Epoch 14/20\n",
      "1316s - loss: 0.0148 - acc: 0.9955 - val_loss: 0.2179 - val_acc: 0.9495\n",
      "Epoch 15/20\n",
      "1316s - loss: 0.0136 - acc: 0.9954 - val_loss: 0.2280 - val_acc: 0.9505\n",
      "Epoch 16/20\n",
      "1316s - loss: 0.0119 - acc: 0.9959 - val_loss: 0.2493 - val_acc: 0.9450\n",
      "Epoch 17/20\n",
      "1316s - loss: 0.0129 - acc: 0.9958 - val_loss: 0.1791 - val_acc: 0.9590\n",
      "Epoch 18/20\n",
      "1316s - loss: 0.0126 - acc: 0.9955 - val_loss: 0.2121 - val_acc: 0.9510\n",
      "Epoch 19/20\n",
      "1316s - loss: 0.0130 - acc: 0.9958 - val_loss: 0.2046 - val_acc: 0.9480\n",
      "Epoch 20/20\n",
      "1316s - loss: 0.0122 - acc: 0.9956 - val_loss: 0.2283 - val_acc: 0.9490\n"
     ]
    }
   ],
   "source": [
    "'''Lower the learning rate and training the network for 20 more epochs.'''\n",
    "tf_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-5, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "nb_epoch=20\n",
    "hist_little_convet = tf_model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch = nb_train_samples,\n",
    "        nb_epoch = nb_epoch,\n",
    "        validation_data = validation_generator,\n",
    "        nb_val_samples = nb_validation_samples,\n",
    "        verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the best validation accuracy achieved was 95.90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the accuracy on train and validation set again:\n",
    "accuracies = np.array([])\n",
    "losses = np.array([])\n",
    "\n",
    "i=0\n",
    "for X_batch, Y_batch in validation_generator:\n",
    "    loss, accuracy = model.evaluate(X_batch, Y_batch, verbose=0)\n",
    "    losses = np.append(losses, loss)\n",
    "    accuracies = np.append(accuracies, accuracy)\n",
    "    i += 1\n",
    "    if i == 20:\n",
    "       break\n",
    "       \n",
    "print(\"Validation: accuracy = %f  ;  loss = %f\" % (np.mean(accuracies), np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
